\chapter{Proposed Methods}
In this chapter, we introduce the proposed methods and experimental methodology. This research aims to address the question, "What would happen if we change the masking strategies?" While numerous strategies have been proposed to enhance the representation of text and images, there has been limited experimentation on altering the masking ratio in masked language modeling (MLM). This study primarily focuses on two main aspects:

\begin{itemize}
  \item The referenced model employs masked language modeling to train feature extraction, maintaining a constant masking ratio throughout the training process. This research proposes varying the masking ratio and introducing a time-variant component during training.
  \item The referenced model includes a function known as momentum-based replace token detection, which also operates with a specific masking ratio. This study examines whether adjusting the masking ratio for this task, similar to the approach taken with MLM, improves prediction performance when both parameters are modified.
\end{itemize}

\color{WIP}
The purpose of this experiment is to examine the hypothesis that time variant masking ratio affect the prediction on text based person retrieval.
This is carreid out by investigating the relation and sensitivity aware representation learning \cite{Bai2023RaSaRA} that investigated the better representation learning for image and text inputs by detecting the replaced tokens from the converted text and corresponding image inputs. The results showed significant improvements in prediction performance. To investigate further towards the masking ratio, \cite{wettig-etal-2023-mask} investigated that larger models should adopt a higher masking rate rather than masking 15\% of tokens conventionally. Another mehtod from Dongjie Yang, et al, \cite{yang2023learningbettermaskingbetter} proposed time-variant masking ratio decay strategy and POS-tagging weighed masking. In results, the time variant masking decay method outperformed the time invariant masking ratio for F1 score on SQuAD performance during pre-training on BERT-large model. 

In this experiment, we investigate the effect of the performance towards the time variant masking ratio on replaced token detection task. To evaluate the performance, we would evaluate the precision rank of 1,5,and 10. To see how the words are attention to, we would use attention visualizer to discuss the affect of where each of the word is attentioning to the image.

Predicting more means the model learns from more training signals, so higher prediction rates boost the model performance. From another perspective, each prediction at each masked token leads to a loss gradient, which is averaged to optimize the weights of the model. Averaging across more predictions has a similar effect to increasing the batch size, which is proved to be beneficial for pre-training (Liu et al., 2019). 

corruption rate how much we mask the words from the text
prediction rate how much we predict the mask token 




what i did 
- change the mask 
- visualize the attention section

The structure of the algorithm is done as follows. 
- image encoder
  mlp 
- text encoder
  bert 

When we work on fine tuning, we change the masking ratio on the mlm process. 
the masking ratio will be changed as follows.
flat 
cosine curve 
random 

from these papers, bert scored higher accuracies when using 40\% on masking ratio. This is basically done in text information, 
so we would like to try it out in text based image retireval tasks. T

flat and cosine will work on the range of 40\% and 60\% 
the previous work has been done as 15\%, so we would compare the results with that for the discussion 

\section{Requirements specification}

- model
rasa

- dataset 
CUHK-dataset
ICFG-dataset

- epoch
30

- learning rate 
1e-5 to 1e-6

\section{Attention visualization}
To be able to see where the model is paying attention to, we would use the attention matrix in cross atttention module.



\section{Algorithm implementation}

for the 