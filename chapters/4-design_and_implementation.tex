\chapter{Proposed Methods}

% do i need this
% In this chapter, we introduce the proposed methods and experimental methodology. This research aims to address the question, "What would happen if we change the masking strategies?" While numerous strategies have been proposed to enhance the representation of text and images, there has been limited experimentation on altering the masking ratio in masked language modeling (MLM). This study primarily focuses on two main aspects:

% \begin{itemize}
%   \item The referenced model employs masked language modeling to train feature extraction, maintaining a constant masking ratio throughout the training process. This research proposes varying the masking ratio and introducing a time-variant component during training.
%   \item The referenced model includes a function known as momentum-based replace token detection, which also operates with a specific masking ratio. This study examines whether adjusting the masking ratio for this task, similar to the approach taken with MLM, improves prediction performance when both parameters are modified.
% \end{itemize}
%%%

% \color{WIP}
% The purpose of this experiment is to examine the hypothesis that time variant masking ratio affect the prediction on text based person retrieval.
% This is carried out by investigating the relation and sensitivity aware representation learning \cite{Bai2023RaSaRA} that investigated the better representation learning for image and text inputs by detecting the replaced tokens from the converted text and corresponding image inputs. The results showed significant improvements in prediction performance. To investigate further towards the masking ratio, \cite{wettig-etal-2023-mask} investigated that larger models should adopt a higher masking rate rather than masking 15\% of tokens conventionally. Another method from Dongjie Yang, et al, \cite{yang2023learningbettermaskingbetter} proposed time-variant masking ratio decay strategy and POS-tagging weighed masking. In results, the time variant masking decay method outperformed the time invariant masking ratio for F1 score on SQuAD performance during pre-training on BERT-large model. 

The purpose of this experiment is to examine the hypothesis that a time-variant masking ratio would affects the prediction accuracy in text-based person retrieval tasks. We approach this by exploring the relationship between the sensitivity-aware representation learning, as investigated by Bai et al. (\cite{Bai2023RaSaRA}) in their work "RaSa," which focuses on improving representation learning for image and text inputs by detecting replaced tokens from the combined text and image inputs. Their results demonstrated significant improvements in prediction performance. They used masked language modeling to combine the image and text feature to match the information, but the masking ratio were fixed to 15\% during the whole process.

To delve deeper into the impact of masking ratios, (\cite{wettig-etal-2023-mask}) suggested that larger models benefit from adopting a higher masking rate rather than the  conventional 15\% of tokens. Additionally, (\cite{yang2023learningbettermaskingbetter}) proposed a time-variant masking ratio decay strategy along with POS-tagging weighted masking. Their findings indicated that the time-variant masking decay method significantly outperformed the time-invariant masking ratio in terms of F1 score on the SQuAD dataset during the pre-training of the BERT-large model.

Our experiment aims to build on these insights by evaluating the effects of different masking strategies on the performance of vision-language models, specifically in the context of text-based person retrieval. By systematically varying the masking ratio and employing time-variant masking techniques, we seek to determine the optimal approach for enhancing the alignment and interpretation of textual and visual data.

% this part might not required
In this experiment, we investigate the effect of the performance towards the time variant masking ratio on replaced token detection task. To evaluate the performance, we will compute the feature similarity score for all image-text pairs. Then we take top-$k$ candidates and calculate their ITM score $s_{itm}$ for ranking. 

\section{Dataset}

\begin{figure}[htbp]
  \begin{center}
      \includegraphics[width=\linewidth]{img/cuhk_pedes.png}
      \caption{Example sentence with corresponding image}
      \label{fig:cuhk_pedes}
  \end{center}
\end{figure}


Dataset we are using is CUHK-PEDES \cite{li2017personsearchnaturallanguage}. This is a large-scale dataset created to facilitate person search using natural language descriptions. It addresses the need for a dataset where persons are described in detail using natural language, enabling practical applications such as video surveillance. Key features of the dataset are shown as follows:
\begin{itemize}
  \item Large-scale: Dataset contains 40,206 images over 13,003 persons 
  \item Annotations: Each images is described with two sentences by independent annotators, in total of 80,412 sentence descriptions
  \item Source diversity: Images are sourced from a variety of existing person re-identification datasets, including CUHK03\cite{li2014deepreid}, Market-1501\cite{7410490}, SSM\cite{ssm}, VIPeR\cite{viper}, and CUHK01\cite{li2012human}.
\end{itemize}

The datasets contains image and corresponding natural language description as shown in Fig\ref{fig:cuhk_pedes}. Image source for CUHK-PEDES are from CUHK03, Market-1501, SSM, VIPER, and CUHK01. The annotations for each image are done by Amazon Mechanical Turk(AMT), which is a crowd workers that are employed to describe each image with two sentence. Each sentence include rich descriptions about person appearance, action poses, and interactions.

\section{Methods}
In this section, we introduce the experimental methods designed to evaluate the performance of vision-language models (VLMs). Recent state-of-the-art VLMs, such as the one described by Bai et al. (2023) in their work "RaSa," utilize an attention mechanism to extract and align features from both images and text using cross-modality encoders. Achieving optimal results necessitates not only robust feature extraction but also the development of sophisticated textual representations to enhance the interpretation of visual data. Therefore, our objective is to train the visual-language model (Bai et al., 2023) by employing diverse strategies for textual information representation. Specifically, we will investigate two distinct methods to determine their impact on model performance.

% explain about the parameters based on the model image and loss function

\subsection{Masking ratio for masked language modeling} 
A Masked Language Model (MLM) is a type of neural network model used primarily for natural language processing tasks, including (\cite{devlin2018bert}, \cite{Bai2023RaSaRA}). MLMs are trained on text data where some of the words in the input sentences are replaced with a special token (often "[MASK]"), and the model's objective is to predict the original words that were masked. This training method helps the model learns to capture the semantics and syntax of the language, making it effective for various NLP tasks such as text classification, sentiment analysis, and machine translation.

During training, a portion of the input tokens are randomly masked, and the model learns to predict these masked tokens based on the surrounding context. For example, in the sentence "The cat sat on the [MASK]," the model should predict "mat" if it has learned the context correctly. For each image and text pair ($I,T^S$), MLM loss will be formulated as:
\[
  L_{mlm} = \mathbb{E}_{p \left( I,T^{msk}\right) }\mathrm{H}\left(y^{mlm}, \phi^{mlm}\left(I,T^{msk}\right)\right)
\]

Given a masked text \( T_{msk} \), where each token in the input text \( T_s \) is randomly masked with a probability \( p_m \), \( y_{mlm} \) represents a one-hot vector indicating the ground truth of the masked token. The function \( \phi_{mlm}(I, T_{msk}) \) denotes the predicted probability for the masked token, based on the context provided by the masked text \( T_{msk} \) and the paired image \( I \).

% explanation about what we do with masking ratio
In natural language pre-training, masked language modeling is utilized to understand the context of an input sentence. This same task is applied to recent vision-language models, where the masked token is predicted using both the image and the remaining text information. Although this method can yield good performance, the typical ratio for masking words in a sentence is fixed at 15\%. In our study, we aim to identify the optimal masking ratio by experimenting with time-invariant masking ratios ranging from 15\% to 40\%. Additionally, we will explore time-variant masking strategies based on previous findings to further enhance model performance.

% - モデルとパラメータの具体的な説明
% - 始めにどの比率が効果的かの検証
% - 次に時間変動での検証

\subsubsection{fixed masking ratio}
We will train the model with constant masking ratio with range of 15\% to 40\% by 5\% steps with the dataset of CUHK-PEDES. Then, from the results, we will take the most highest mAP from the constant masking ratio and use that value with the minimum masking ratio. The minimum ratio will be 2\% since 0\% masking ratio will not train at all. 

For all training tasks, the number of epochs is set to 30, batch size is 13. With this, the model will train under 30 iterations and trains with the dataset which is divided to 13 different datasets. The optimization methods are adamW, learning rate is set to 1.0e-4 at first and create a cosine curve to decrease to 1.0e-5. 

\begin{tabular}{rcccc}
  masking ratio & r1 & r5 & r10 & mAP\\ \hline
  15\% & 76.51 & 90.20 & 94.25 & 69.38 \\
  20\% & 76.64 & 89.90 & 93.70 & 70.27 \\
  25\% & 76.23 & 89.86 & 93.79 & 70.22 \\
  30\% & 76.72 & 89.77 & 93.60 & 70.30 \\
  35\% & 76.61 & 89.77 & 93.47 & 70.26 \\
  40\% & 76.56 & 89.88 & 93.47 & 70.26
\end{tabular}

\subsubsection{time variant masking ratio}
Time variant strategy will have the cosine curve and linear curve to see the result. The time variant will have four different methods, linear increase, linear decrease, cosine increase, and cosine decrease. The equations are shown as follows:
\begin{displaymath}
  M_{linear increase}(t) = P*t/T \
  M_{linear decrease}(t) = P*(1-t/T) \
  M_{cosine increase}(t) = P*(1-cos(\pi*t/T)) + 0.02 \ 
  M_{cosine decrease}(t) = P*(1+cos(\pi*t/T)) + 0.02 
\end{displaymath}

\begin{tabular}{rcccc}
  masking ratio & r1 & r5 & r10 & mAP\\ \hline
  linear increase & 76.62 & 89.88 & 93.62 & 70.36 \\
  linear decrease & 76.70 & 89.85 & 93.58 & 70.21 \\
  cosine increase & 76.54 & 90.04 & 94.02 & 70.15 \\
  cosine decrease & 76.21 & 90.09 & 93.65 & 70.21 \\
\end{tabular}

\section{masking ratio for replaced token detection}
RaSa utilizes this to create a task called sensitivity aware learning.
Sensitivity aware aims to make the model sensitive to specific transformations in the data, particularly textual changes. While many models aim to learn invariant representations that are robust to various data augmentations, RaSa takes it further by encouraging the model to detect and respond to sensitive transformations, such as word replacements in text. This is done using a Momentum-based Replaced Token Detection (M-RTD) task, where the model learns to detect whether a token in the text has been replaced. This sensitivity to changes enhances the robustness and discrimination power of the representations learned by the model.


Predicting more means the model learns from more training signals, so higher prediction rates boost the model performance. From another perspective, each prediction at each masked token leads to a loss gradient, which is averaged to optimize the weights of the model. Averaging across more predictions has a similar effect to increasing the batch size, which is proved to be beneficial for pre-training (Liu et al., 2019). 

\section{Attention visualization}
To be able to see where the model is paying attention to, we would use the attention matrix in cross atttention module.



\section{Algorithm implementation}

for the 