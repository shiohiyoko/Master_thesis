\chapter{Literature Review}


% ------------------------------------------------------------ %
\section{Research Methodology}
In the methodology section, the we first delves into the existing literature, drawing from a paper accessible through the platform "Paper with Code." This platform typically provides research papers along with their associated code implementations. The chosen paper appears to be selected based on its prominence, likely measured by its reported accuracy or success in the field.
Following the identification of the primary paper, the researcher conducts a thorough review of its content, focusing particularly on aspects related to methodology. This involves understanding the proposed techniques, algorithms, and approaches presented in the paper to achieve high accuracy in the context of text-based person searches. The aim is to comprehend the nuances of the existing methodology and identify the key factors contributing to its success.
In addition to the primary paper, the researcher examines two other papers that exhibit a significant difference in accuracy. This comparative analysis is valuable for gaining insights into different approaches within the field. The choice of these additional papers may be strategic, aiming to capture diverse perspectives or methodologies, especially if there is a notable contrast in their reported accuracy metrics.
The researcher likely scrutinizes the methodologies of these selected papers, comparing and contrasting them with the primary paper. This comparative analysis helps identify the strengths and weaknesses of different approaches, shedding light on potential areas of improvement or innovation for the current research.
Overall, the methodology involves a comprehensive exploration of relevant literature, with a focus on the primary paper selected from "Paper with Code." The intent is to understand the methodologies employed in achieving high accuracies and to leverage insights from other papers with varying performance metrics. 

However, if only paperwithcode is used, the information obtained is limited and biased. To eliminate this bias, we decided to use scopus to search a wider range of papers by keyword search.

\subsection*{Identification}

The following research question was defined:

\bigskip
\textit{``How does the model compares with the text information with the image information''}
\bigskip



From this research question, four main keywords that sufficiently explain the topic were used: person retrieval and vision language pre-training.
Furthermore, synonyms and related terms were associated to these keywords to form keyword groups as follows:

\begin{itemize}
    \item person retrieval:
    \begin{itemize}
        \item person;
        \item person detection;
        \item person search.
    \end{itemize}
    \item vision language pre-training:
    \begin{itemize}
        \item VLP;
        \item text based;
        \item text.
    \end{itemize}
\end{itemize}


From the keywords, we had a keyword search on scopus from the search strings as follows:

\begin{itemize}
    \item ( "person retrieval" OR "person" OR "person detection" OR "person search" ) AND ( "vision language pre-training" OR "VLP" OR "text based" OR "text" ).
\end{itemize}

The Scopus search yielded a total of $20170$ documents. Within this result, we set the subject area to Computer Science, document type to article and conference paper, language to only english, and set the open access to all open access. With this filters, $862$ articles were found. 

\subsection*{Screening}

Various factors were taken into account for the exclusion of documents:
\begin{enumerate}
    \item problem and goal were too different (e.g., building new hardware, analysis of leaf reflectance);
    \item not sufficiently related to this work (e.g., focused on hyperspectral );
    \item duplicates that were not automatically detected and excluded.
\end{enumerate}


\subsection{Learning Strategies}
Contrastive learning aims to map input images and texts to the same feature space such that the distance between the embeddings of image-text pairs is minimized if they match or maximized if they donâ€™t. This method is a commonly used pre-training objectives for vision models and proven to be a highly effective for vision-language models as well. \cite{radford2021learning} uses this learning strategy with a cosine distance between the text and image embeddings. For pre-training methods requires large datasets to train, so most of the times, they use image and corresponding caption from the internet to train. This way, the model can train with large data, but in the other hand the image and caption sometimes does not correlate. To deal with this problem, ALIGN\cite{jia2021scaling} and DeCLIP\cite{li2022supervision} designed their own distance metrics.
% ------------------------------------------------------------ %

\section{Transformer}

% general explanation of transformer
% Transformer is an encoder-decoder model which is made with attention mechanism \cite{vaswani2023attention}. This model was first created to solve the problem of "lack of understanding the global nuances" and "faster training speed" which the Seq2Seq had. The author approach to this problem by applying a mechanism called "attention" to understand the caution expression. This is similar to gate mechanism in LSTM where they emphasizes the important information, and make unnecessary information inconspicuous. By this method, the model can focus to the important information from the enormous information to process. 

The Transformer model, introduced by \cite{vaswani2023attention}, is an encoder-decoder architecture featuring an attention mechanism. Initially devised to address the limitations of the Seq2Seq model, such as the challenges in grasping global nuances and the need for faster training, the Transformer model leverages attention mechanisms. These mechanisms, akin to the gating mechanism found in LSTM networks, prioritize important information while suppressing irrelevant details. By employing this approach, the model can effectively discern crucial information from the vast array of input data for processing.

Structure of the Transformer is shown figure\ref{fig:transformer}. The model is structured as encoder-decoder model, most competitive neural sequence transduction models are also encoder-decoder model, with encoder maps an input of symbol representation $(x_1, ...,x_n)$ toa continuous representation $Z=(z_1,...,z_n)$. The decoder will take the given z to generate an output sequence of $(y_1,...,y_n)$ of symbols one at a time.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=10cm]{img/transformer.eps}
        \caption{Structure of the Transformer}\label{fig:transformer}
    \end{center}
\end{figure}

% % talk about transformer block, plus multihead attention
% Transformer Block is the unit for transformer, and this contains multi-head attention and position wise feed-forward networks. Transformer block has three input Query, Key, and Value which is used for computing attention representation on Multi-head attention.
% The position-wise feed-forward networks converts the output of Multi-head Attention to for each tokens. Skip connection is placed between the layers to decrease the gradient loss, which enables deepening the model. 

% % image of transformer block with q,k,v input

% To explain Multi-Head attention, we first describe the Single-Head Attention. Single-head Attention is a Attention module with only one Scale-Dot Product. 

\section{Vision-language pre-training}

Vision and Language(VL) is a major research area for the causality of Computer Vision and Natural Language Processing (NLP), which aims to effectively learn from multimodal data. Some of the great success of language model pre-training in NLP(RoBERTa(\cite{liu2020roberta}), BERT(\cite{devlin2018bert}), and GPT-3(\cite{brown2020language})) influenced the field of Vision-Language Pre-training (VLP) to grow attention from both fields.Towards this tasks, many studies have been proposed.

% The existing methods for VLP rely on object detection models, multiple auxiliary objectives, and limited-scale human annotation data. To solve this challenges, 

\begin{itemize}
    \item \textbf{VisualBERT}\\
    A joint representation model for VL task which is designed to capture rich semantics in the image and associated text. The author integrated BERT for the VLP task with pretrained object proposal systems like Faster-RCNN. Figure\ref{fig:visualvert} shows how the model is trained. VisualBERT requires the input of image regions and language to train with a masked language modeling and sentence-image prediction task. With these tasks, the model will realize the implicit alignment between the image and text representation.
    \begin{figure}[htbp]
        \begin{center}
            \includegraphics[width=10cm]{img/VisualBERT.eps}
            \caption{Representation of VisualBERT}\labzel{fig:visualvert}
        \end{center}
    \end{figure}

    \item \textbf{SimVLM}\\
    Simple visual language model(SimVLM), introduced by \cite{wang2022simvlm}, is a minimalist pretraining framework for joint visual and textual representation. The motivation for this research is the limitations and complexity of existing vision-language pretraining models, which often require expensive annotations and multiple dataset-specific objectives. SimVLM aims to address these issues by utilizing large-scale weak supervision and training end-to-end with a single prefix language modeling objective.


    \item \textbf{CLIP}\\
    CLIP, introduced by \cite{radford2021learning}, is build on a vision transformer architecture, similar to BERT but instead of joint representation model, CLIP uses different encoder for both modal and from the extracted features, we calculate the dot product. During the learning process, the diagonal component of this matrix are learned to have larger values than the others.
    
\end{itemize}


\section{Person Understanding Task}
in this section we will talk about different method we can use for person understanding task. 

\subsection{Text-based re-identification}
This section presents papers that study text-based person retrieval. A common issue addressed in each paper is the deficiency of the feature from text and image encoders. It has been confirmed that when the features of each modal are integrated, information is distorted or missing, which affects the accuracy of detection. Therefore, how to resolve this deficiency is key in this section.

\begin{itemize}
    \item \textbf{Relation and Sensitivity Aware representation learning}\\
    This paper introduces a method called Relation and Sensitivity Aware representation learning (RaSa) that includes two novel tasks: Relation-Aware learning (RA) and Sensitivity-Aware learning (SA). It addresses the shortcomings of existing methods in text-based person search, where clustering representations of positive pairs without distinction leads to overfitting, particularly with weak positive pairs. Fig\ref{fig:rasa} represents the overall structure of RaSa. RA mitigates overfitting by introducing a positive relation detection task to distinguish between strong and weak positive pairs. Additionally, the author emphasizes the common practice of learning invariant representation under data augmentation for robustness but goes further by encouraging the representation to perceive sensitive transformations through SA, promoting enhanced robustness by detecting replaced words in textual descriptions.
    \begin{figure}[htbp]
        \begin{center}
            \includegraphics[width=10cm]{img/rasa.eps}
            \caption{Structure of RaSa}\label{fig:rasa}
        \end{center}
    \end{figure}
    
    \item \textbf{Implicit Relation Reasoning and Aligning}\\
    The paper introduces a novel approach, called IRRA (Implicit Relation Reasoning and Aligning), for text-to-image person retrieval. This task involves identifying a person based on a given textual description. The main challenge is to establish an effective mapping between visual and textual modalities in a shared latent space as shown in Fig\ref{fig:irra}. Unlike previous methods that use separately pre-trained unimodal models, IRRA addresses this challenge by introducing a cross-modal Implicit Relation Reasoning module. This module integrates visual cues into textual tokens through a masked language modeling paradigm, facilitating cross-modal interaction. To globally align visual and textual embeddings, the paper proposes Similarity Distribution Matching, which minimizes the KL divergence between image-text similarity distributions and normalized label matching distributions. 
    \begin{figure}[htbp]
        \begin{center}
            \includegraphics[width=10cm]{img/irra.eps}
            \caption[short]{The progression of paradigms in text-to-image person retrieval is as follows: (a) Early methods. Global-matching methods directly synchronize global image and text embeddings. (b) Recent methods. Local-matching methods have emerged, focusing on the explicit extraction and alignment of local image and text embeddings. (c) Presented method. Rooted in implicit relation reasoning, enhances the alignment of global image and text embeddings by implicitly considering the relationships among all local tokens.}\label{fig:irra}
        \end{center}
    \end{figure}

    \item \textbf{Semantic-Aligned Feature Representation}\\
    The paper (\cite{li2021learning}) focuses on text-based person search, aiming to retrieve images of a specific pedestrian based on a textual description. The primary challenge in this task is to bridge the inter-modality gap and align features across textual and visual modalities. The proposed solution is a semantic-aligned embedding method that automatically learns feature alignment between visual and textual representations shown in Fig\ref{fig:sem_align}. The method utilizes two Transformer-based backbones to encode robust feature representations for images and texts. Additionally, a semantic-aligned feature aggregation network is introduced, incorporating a multi-head attention module constrained by a cross-modality part alignment loss and a diversity loss. 

    \begin{figure}[htbp]
        \begin{center}
            \includegraphics[width=10cm]{img/sem_feature.eps}
            \caption{Structure of presented model}\label{fig:sem_align}
        \end{center}
    \end{figure}
\end{itemize}

% \subsection{Image-based re-identification}
% with the image based methods, we have the old fashion way of cnn 

\subsection{Additional supervision}
person attribute recognition has seeked to have supervision 


\begin{itemize}
    \item \textbf{A Simple and Robust Correlation Filtering}\\
    The research paper (\cite{Suo_ECCV_A}) presents a novel Simple and Robust Correlation Filtering (SRCF) method for text-based person search, aiming to extract differentiated representations and align them among identities and descriptions. The method introduces denoising filters and dictionary filters to establish multimodal mappings and improve the robustness of the model. Extensive experiments conducted on text-based person search datasets demonstrate the method's performance improvements. 
    
    \item \textbf{Vision-Guided Semantic-Group Network} \\
    The research paper (\cite{he2023vgsg}) proposes a Vision-Guided Semantic-Group Network (VGSG) for text-based person search, aiming to align well-grained visual and textual features to retrieve images based on textual descriptions. The proposed VGSG includes Semantic-Group Textual Learning (SGTL) and Vision-guided Knowledge Transfer (VGKT) modules. SGTL groups textual features based on semantic cues of language expression, while VGKT aligns semantic-group textual features with visual features. 
    
    \item \textbf{Pose-Guided Multi-Granularity Attention Network}\\
    The research paper (\cite{jing2019poseguided}) proposed a pose-guided multi-granularity attention network for text-based person search, addressing the challenges in cross-modal matching and the importance of considering different granularities of semantic relevance between images and descriptions. The paper introduced a coarse alignment network (CA) to select related image regions to the global description and a fine-grained alignment network (FA) to capture phrase-related visual body parts using pose information. 
    
\end{itemize}
% ------------------------------------------------------------ %
