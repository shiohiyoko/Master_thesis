\chapter{Related Studies }
{\color{red} need to think about this a little further}
To provide a comprehensive understanding of the context and foundation for this research, we review related studies that have explored multi-modal representation and the impact of masking ratios in Vision-Language Models.

% ------------------------------------------------------------ %
{\color{red}maybe not required}
\section{Research Methodology}
We first delve into the existing literature, drawing from papers accessible through the "Paper with Code" platform. This platform typically provides research papers along with their associated code implementations. {\color{red}This papers was selected based on its prominence, measured by the reported accuracy and success in the field, as stated in the platform.}

Following the identification of the primary paper, the researcher conducts a thorough review of its content, focusing particularly on aspects related to methodology. This involves understanding the proposed techniques, algorithms, and approaches presented in the paper to achieve high accuracy in the context of text-based person searches. The aim is to comprehend the nuances of the existing methodology and identify the key factors contributing to its success.

In addition to the primary paper, the researcher examines two other papers that exhibit a significant difference in accuracy. This comparative analysis is valuable for gaining insights into different approaches within the field. We aim to investigate diverse perspectives and approaches, especially if there is a notable contrast in reported accuracy metrics.
We scrutinizes the methodologies of these selected papers, comparing and contrasting them with the primary paper. This comparative analysis helps identify the strengths and weaknesses of different approaches, shedding light on potential areas of improvement or innovation for the current research.

Overall, the methodology involves a comprehensive exploration of relevant literature, with a focus on the primary paper selected from "Papers with Code." The intent is to understand the methodologies employed in achieving high accuracies and to leverage insights from other papers with varying performance metrics. 

However, if only Papers with Code is used, the information obtained is limited and biased. To eliminate this bias, we decided to use the Scopus database to search for a wider range of papers by keyword search.

\subsection*{Identification}
From this research question, two main keywords that sufficiently explain the topic were used: person retrieval and vision language pre-training.
Furthermore, synonyms and related terms were associated to these keywords to form keyword groups as follows:

\begin{itemize}
    \item person:
    \begin{itemize}
        \item retrieval;
        \item detection;
        \item search.
    \end{itemize}
    \item vision language pre-training:
    \begin{itemize}
        \item VLP;
        \item text.
    \end{itemize}
\end{itemize}


From the keywords, we had a keyword search on scopus from the search strings as follows:

\begin{itemize}
    \item ( "person" AND ("retrieval" OR "detection" OR "search") ) AND ( "vision language pre-training" OR "VLP" OR "text" ).
\end{itemize}

The Scopus search yielded a total of $20170$ documents. Within this result, we set the subject area to Computer Science, document type to article and conference paper, language to only english, and set the open access to all open access. With this filters, $862$ articles were found. 

\subsection{Screening}

Various factors were taken into account for the exclusion of documents:
{\color{red} changes required}
\begin{enumerate}
    \item problem and goal were too different (e.g., building new hardware, analysis of leaf reflectance);
    \item not sufficiently related to this work (e.g., focused on hyperspectral );
    \item duplicates that were not automatically detected and excluded.
\end{enumerate}

% ------------------------------------------------------------ %

\section{Vision-language Models}
Vision-language models have been researched significantly by leveraging end-to-end trainable deep neural networks (DNN). Before the vision-language model, both vision and language were researched with different DNN model structures. 

The field of vision recognition has been rapidly evolving in recent years. Until now, the vision recognition models have experienced 5 stages (\cite{zhang2024visionlanguagemodelsvisiontasks}): traditional machine learning, deep learning from scratch, supervised pre-training with fine-tuning, unsupervised pre-training with fine-tuning, and vision-language model with pre-training.

Traditional machine learning relies on feature engineering. To achieve features, general methods use hand-craft features (\cite{svmclassification}) and lightweight models (\cite{knn, svm}) to classify images into predefined categories. However, this method demands domain experts to design effective features for specific visual recognition tasks, which makes it ineffective for complex tasks and limits its scalability.

To overcome the problems with traditional methods, deep learning methods were devised (\cite{imagenet, dnn_imagerecognition}) to enhance feature engineering and allowed to focus on the architecture engineering of neural networks to learn features effectively. Great success came from ResNet (\cite{resnet}) with the introduction of residual connections as a solution to the gradient vanishing problem, achieving unprecedented performance.However, this approach faced issues such as slow training convergence and the need for extensive labeled data. 

Recent studies (\cite{radford2021learning}) have discovered that the features learned from large-scale labeled datasets possess a high degree of versatility and can be effectively transferred to a variety of downstream tasks. This indicates that the patterns and representations captured during the training on these extensive datasets are not only specific to the initial context but are also generalizable, allowing them to be utilized in different, often more specialized applications. The transferability underscores the potential of leveraging pre-trained models to enhance performance and efficiency in a wide range of tasks across various domains. 
These learning techniques accelerates training models with limited task-specific training datasets while achieving high performance.

{\color{red}supervision}

Supervised pre-training outperformed many of the previous state-of-the-art performance on visual recognition tasks, yet still has the problem of preparing large-scale labeled datasets. To overcome the dependency on labeled data, \cite{he2020momentumcontrastunsupervisedvisual} presented a method called Momentum Contrast (MoCo), which builds a dynamic dictionary with a queue and a moving-averaged encoder, enabling the model to learn effective representation without labeled data. Other methods are Simple Framework for Contrastive learning (SimCLR) introduced by \cite{chen2020simpleframeworkcontrastivelearning}. This is a simplified contrastive learning framework for visual representation, avoiding specialized architectures or memory banks. Beyond these foundations, pre-training models no longer rely on labeled datasets, which enables to learn from various training data compared to supervised pre-training. 

Drastic improvements have emerged from natural language processing (\cite{devlin2018bert, brown2020language}), combined with previous vision recognition models. Specifically, a new paradigm called vision-language model pre-training has been proposed for vision recognition. To pre-train, this model does not require labeled datasets, but instead image-text pairs. With this, the training datasets can be found infinitely on the internet. However, the image-text pair on the internet contains information that does not correlate to each other. To train the model with noisy data, VLM is trained with certain vision-language objectives (\cite{radford2021learning, yu2022cocacontrastivecaptionersimagetext}). From these objectives, the VLM matches the embedding of any given images and text, which enables the performance of zero-shot prediction without fine-tuning on downstream visual recognition.

% Vision-Language models are a model that combines both the vision and language modalities and enables to process both information. 
% Take, for example, the task of zero-shot image classification. Weâ€™ll pass an image and a few prompts like so to obtain the most probable prompt for the input image.
% To predict the probable prompt, the model needs to understand both input image and the text prompts. To understand those modalities, the model will have separate or fused encoders for both vision and language.

\section{Masking Strategies}
Learning robust representation from image and text is the key challenge for VLM. An approach to this challenge is previously done with masked language modeling. This is a pre-training task for language models, where the model learns to predict masked tokens in a sequence based on the surrounding context. This strategy is foundational for several state-of-the-art models (\cite{devlin2018bert, liu2020roberta}). 

In MLM, it is necessary to develop a strategy to mask the text. Key masking strategy are random masking, span-based masking, whole word masking, and entity masking. 

Random masking are done by randomly select the tokens from the sentence and replaces them with special token, [MASK]. For example, in BERT's pre-training, 15\% of the tokens are masked, out of which 80\% are replaced with [MASK], 10\% are replaced with random tokens, and remaining 10\% unchanged. 

To understand longer text spans, span-based masking was conceived. Span-based masking involves masking contiguous spans of tokens, with this, the model will be encouraged to learn dependencies across longer text spans. 

Another method is whole word masking. This strategy masks entire words instead of sub-word units. If any part of a word is selected for masking, the entire word is masked.

In VLM, similar masking strategies can be employed to enhance the model's ability to understand and generate language in relation to visual content.

Text caption masking is a masking strategy to mask the words or phrases in the text caption. It is used in VLM training to reconstruct the masked tokens based on the visual content and the unmasked text. This approach leverages the contextual understanding from both modalities. ImageBERT (\cite{qi2020imagebertcrossmodalpretraininglargescale}) used masked language modeling on text captions to pre-train the VLM.

Another method is visual patch masking: patches of an image are masked and the VLM is trained to reconstruct the missing parts based on the unmasked image regions and the associated text.

\section{Model Used In This Thesis}
In this section, vision language model used in this thesis is introduced.

\subsection{ALBEF: Align before Fuse}
Text encoder and image encoders are used to learn the representation in vision-and-language pre-training models (VLP). Most existing VLP models use multi-modal encoders to jointly model visual tokens (image features) and word tokens (text features). Although this method gives a certain level of performance, it faces challenges if the visual and word tokens are unaligned, making it difficult for the encoder to learn interactions between them. 
To tackle this challenge, \cite{li2021align} first aligns image and text representations before fusing them through cross-modal attentions. 

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\linewidth]{img/albef_model_structure.png}
        \caption{Structure of ALBEF: Souce from \cite{li2021align}}
        \label{fig:albef}
    \end{center}
\end{figure}

The author represented a model with an image encoder, text encoder, and a multi-modal encoder. They use 12-layer visual transformer ViT-B/16 (\cite{dosovitskiy2021image}), pre-trained with ImageNet-1k, as the image encoder. 
Text encoder is pre-trained by the first 6 layers of $BERT_{base}$. Multi-modal encoder, fusing text and image features, is pre-trained with the last 6 layers of $BERT_{base}$.
The text encoder transforms an input text $T$ into a sequence of embedding $\{w_{cls}, w_1, ..., w_N\}$. The image features also transform an input image $I$ into a sequence of embedding $\{v_{cls}, v_1, ..., v_M\}$. The text features are fed to a multi-modal encoder, while the image features are fused through cross attention at each layer of the multi-modal encoder.

In the pre-training task, the model has three objectives: image-text contrastive learning (ITC) on the unimodal encoders, masked language modeling (MLM), and image-text matching (ITM).
\cite{li2021align} utilizes the hard negative mined from ITC to improve ITM.

Image text contrastive learning is used to align the representation from the unimodal encoders before fusing. This objective makes it easier for the multi-modal encoder to perform cross-modal learning, as the input features are already aligned in a common space. 

Masked language modeling (MLM) is utilized with both image and text information to determine the masked text. \cite{li2021align} randomly masks 15\% of the input token with specialized token [MASK], which minimizes the cross-entropy loss.

\begin{displaymath}
    L_{mlm} = \mathbb{E}_{(I,\hat{T})~D}H(y^{msk}, p^{msk}(I,\hat{T}))
\end{displaymath}

$\hat{T}$ is masked text, $P^{msk}(I,\hat{T})$ is the model's predicted probability for a masked token. 

Contrastive loss is used for alignment, which helps to ground the vision and text representations.

\subsection{RaSa: Relation and Sensitivity Aware}

Vision language models are very popular regions. Many studies have been published with the goal of binding the visual information with the text information. Although a variety of methods have been proposed, but learning a powerful multi-modal representation is still a challenging task. \cite{Bai2023RaSaRA} approaches to this task by two innovations: Relation-aware learning and Sensitivity-aware learning (\cite{Bai2023RaSaRA}).

\begin{figure}
    \includegraphics[width=\linewidth]{img/weak_positive_relation.png}
    \caption{Illustration of (a) two types of image-text pair in same class. The  }
\end{figure}

The authors remark that the text and image pair of the same ID have a strong positive pair and weak positive pair. Since the textual description is generated by a single image in the text-based person search dataset, the text will strongly correlate to the image, but won't always align well to other images of the same person. Previous methods did not take this intra-variation into account when training, and instead put equal weight for strong and weak positive pairs in learning representations. This led the model to overfitting due to the weak pairs.

To reduce the impact of noise interference from weak positive pairs, the authors introduced a Relation-Aware learning (RA) task. This task consists of a probabilistic Image-Text Matching (p-ITM) component and a Positive Relation Detection (PRD) component. The p-ITM is a variant of the commonly used ITM, designed to differentiate negative and positive pairs by probabilistically considering strong or weak positive inputs. Meanwhile, the PRD explicitly distinguishes between strong and weak positive pairs. In this framework, p-ITM focuses on the consistency between strong and weak positive pairs, whereas PRD emphasizes their differences, effectively acting as a regularization for p-ITM. By incorporating RA, the model can extract valuable information from weak positive pairs through p-ITM and reduce noise interference through PRD, ultimately achieving a balance.

Furthermore, improving the robustness of representations often involves learning invariant representations under a set of manually chosen transformations, referred to as insensitive transformations (\cite{caron2021unsupervisedlearningvisualfeatures}). While this approach is recognized, the authors went further by drawing inspiration from the recent success of equivariant contrastive learning (\cite{dangovski2022equivariantcontrastivelearning}). The authors explore sensitive transformations that would degrade performance if applied to learn transformation-invariant representations. Instead of maintaining invariance under insensitive transformations, they encourage the learned representations to be aware of sensitive transformations. 

To achieve this, the authors proposed a Sensitivity-Aware learning (SA) task. They use word replacement as the sensitive transformation and develop a Momentum-based Replaced Token Detection (m-RTD) pretext task. This task involves detecting whether a token originates from the original textual description or the replacement, as illustrated in Figure 1 (b). The closer the replaced word is to the original one (i.e., the more confusing the word), the more challenging the detection task becomes. Training the model to effectively solve this detection task is expected to enhance its ability to learn better representations.

The authors utilize Masked Language Modeling (MLM) to perform word replacement, leveraging the image and text contextual tokens to predict the masked tokens. Additionally, considering that a momentum model, which is a slow-moving average of the online model, can learn more stable representations than the current online model (\cite{grill2020bootstraplatentnewapproach}), the authors use MLM from the momentum model to generate more confusing words. 

Overall, MLM and m-RTD together form Sensitivity-Aware learning (SA), providing robust surrogate supervision for representation learning.

% In this section we will introduce the masking strategies for masked learning modeling. Existing pre-trained language models tries to have different masking strategies to enhance training efficiency and effectiveness. 
% Joshi, et al.  introduced SpanBERT which is designed to better represent and predict span of text.


