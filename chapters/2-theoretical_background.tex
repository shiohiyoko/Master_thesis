\chapter{Theoretical Background}
This section will describe the theoretical background for the techniques used for text encoder, image encoder, and cross attention methods. 
\section{Transformer}
A transformer is a type of deep learning model architecture that has revolutionized the field of natural language processing (NLP) and has been extended to various other domains such as computer vision and speech recognition. The transformer model was introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017. Here's an overview of its key components and how it works:

 Key Components of a Transformer

1. **Self-Attention Mechanism**:
   - **Self-attention** (or scaled dot-product attention) allows the model to weigh the importance of different words in a sentence relative to each other. It enables the model to focus on relevant parts of the input sequence when generating an output.
   - For each word, self-attention calculates a weighted sum of all the words in the sentence, where the weights are determined by the relevance of the words to each other.

2. **Multi-Head Attention**:
   - Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, the transformer splits the input into multiple heads and performs attention in parallel, then concatenates the results.

3. **Positional Encoding**:
   - Since transformers do not have a built-in notion of the order of elements in a sequence, positional encoding is added to the input embeddings to provide information about the position of each word in the sequence.

4. **Feed-Forward Neural Network**:
   - Each position in the sequence is processed independently through a feed-forward neural network. This is applied identically to each position, providing additional processing power to the model.

5. **Layer Normalization and Residual Connections**:
   - Layer normalization helps in stabilizing and accelerating the training process.
   - Residual connections (or skip connections) allow gradients to flow more easily through the network, which helps in training deep networks.

 Architecture Overview

A transformer model is composed of an encoder and a decoder, both of which are stacks of identical layers:

- **Encoder**: Each layer in the encoder consists of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The input to the encoder is a sequence of word embeddings, along with positional encodings.
  
- **Decoder**: Each layer in the decoder has three sub-layers: a multi-head self-attention mechanism, a multi-head attention mechanism over the encoder's output, and a position-wise fully connected feed-forward network. The decoder generates the output sequence by attending to both the previously generated tokens and the encoder's output.

 Advantages of Transformers

1. **Parallelization**:
   - Unlike recurrent neural networks (RNNs), transformers do not rely on sequential processing, allowing for much more parallelization during training, which significantly speeds up the process.

2. **Handling Long Dependencies**:
   - The self-attention mechanism allows transformers to handle long-range dependencies more effectively than RNNs and LSTMs, which can struggle with distant dependencies due to vanishing gradients.

3. **Scalability**:
   - Transformers have shown to scale well with the availability of large datasets and computational resources, leading to significant improvements in performance as the model size increases.

 Applications

- **Natural Language Processing**: Tasks such as translation, text summarization, and sentiment analysis.
- **Computer Vision**: Vision Transformers (ViTs) have been used for image classification and object detection.
- **Speech Recognition**: Transformers have been applied to model speech data and transcribe audio recordings.

 Popular Transformer Models

- **BERT (Bidirectional Encoder Representations from Transformers)**: Pre-trained on large text corpora to generate contextual embeddings for NLP tasks.
- **GPT (Generative Pre-trained Transformer)**: A series of models designed for generating coherent and contextually relevant text.
- **T5 (Text-To-Text Transfer Transformer)**: Converts all NLP tasks into a text-to-text format, providing a unified framework for diverse tasks.

In summary, the transformer model has become the foundation for many state-of-the-art approaches in NLP and beyond due to its ability to handle complex dependencies and its efficiency in training and inference.

\section{BERT}
BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for natural language processing (NLP) tasks. Developed by researchers at Google and introduced in the paper "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" in 2018, BERT has significantly advanced the state of the art in NLP by providing a powerful pre-trained language representation model. Here's a detailed explanation of BERT:

 Key Concepts of BERT

1. **Bidirectionality**:
   - Traditional language models, like the original GPT, are unidirectional, meaning they predict the next word in a sequence based only on the words that come before it. BERT, however, is bidirectional. It considers both the left and right context when predicting a word, which allows it to better understand the context and semantics of the text.

2. **Transformers**:
   - BERT is built upon the transformer architecture, specifically utilizing the encoder part of the transformer. The transformer’s self-attention mechanism allows BERT to focus on different parts of the input text to understand the relationships between words and phrases better.

 Pre-training and Fine-tuning

BERT involves two main steps: pre-training and fine-tuning.

1. **Pre-training**:
   - **Masked Language Modeling (MLM)**: During pre-training, some percentage of the input tokens are randomly masked, and the model is trained to predict the original tokens based on the context provided by the unmasked tokens. This helps BERT learn bidirectional representations of the text.
   - **Next Sentence Prediction (NSP)**: This task involves predicting whether a given sentence B is the actual next sentence that follows a given sentence A in the corpus. This helps BERT understand the relationships between sentences.

2. **Fine-tuning**:
   - After pre-training, BERT can be fine-tuned on a specific NLP task such as question answering, sentiment analysis, or named entity recognition. During fine-tuning, the pre-trained BERT model is further trained on a labeled dataset for the specific task, adjusting its weights to optimize performance on that task.

 BERT Variants

Several variants of BERT have been developed to cater to different needs:

1. **BERT Base**: Consists of 12 layers (transformer blocks), 768 hidden units, and 12 attention heads.
2. **BERT Large**: Consists of 24 layers, 1024 hidden units, and 16 attention heads.
3. **DistilBERT**: A smaller, faster, and lighter version of BERT, retaining 97% of BERT's language understanding while being 60% faster.
4. **RoBERTa**: Robustly optimized BERT approach, which improves on BERT by training with more data and longer sequences.
5. **ALBERT**: A lite version of BERT, which reduces the model size while maintaining performance by sharing parameters across layers.

 Applications of BERT

BERT has been applied to a wide range of NLP tasks, including:

1. **Question Answering**: BERT can be fine-tuned on datasets like SQuAD to understand and answer questions based on given texts.
2. **Sentiment Analysis**: Determining the sentiment expressed in a piece of text.
3. **Named Entity Recognition (NER)**: Identifying and classifying named entities (e.g., people, organizations, locations) within a text.
4. **Text Classification**: Classifying texts into different categories.
5. **Language Translation**: Assisting in translating text from one language to another.
6. **Text Summarization**: Generating concise summaries of longer texts.

 Impact of BERT

BERT has had a transformative impact on NLP by providing a robust, pre-trained model that can be fine-tuned for a variety of tasks with relatively little labeled data. This has significantly lowered the barrier to achieving state-of-the-art performance on many NLP benchmarks and has spurred further research and development in transformer-based models.

In summary, BERT's introduction of bidirectional context representation, coupled with its robust pre-training on large corpora and the flexibility of fine-tuning for specific tasks, has made it a cornerstone in modern NLP, influencing a wide array of applications and subsequent model developments.


\section{Visual Transformer}
A Visual Transformer (ViT) is a deep learning model designed to handle computer vision tasks using the transformer architecture, which was initially developed for natural language processing (NLP). The concept of using transformers for vision tasks was introduced by researchers at Google in the paper "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" by Dosovitskiy et al., published in 2020. Here’s an overview of the Visual Transformer:

 Key Concepts of Visual Transformers

1. **Patch Embeddings**:
   - Instead of processing entire images directly, ViTs divide an image into a grid of smaller, non-overlapping patches (e.g., 16x16 pixels each).
   - Each patch is flattened into a vector and then linearly projected into a fixed-dimensional embedding space. This process is similar to the tokenization in NLP, where words are converted into embeddings.

2. **Positional Encodings**:
   - Since transformers do not have any built-in understanding of the order or spatial structure of patches, positional encodings are added to the patch embeddings. These encodings provide information about the position of each patch in the original image, allowing the model to capture spatial relationships.

3. **Transformer Encoder**:
   - The core of the ViT model is the transformer encoder, which is composed of multiple layers of multi-head self-attention and feed-forward neural networks. This architecture allows the model to learn complex relationships between different parts of the image.
   - The transformer encoder processes the sequence of patch embeddings and positional encodings to produce a representation of the image.

4. **Classification Token**:
   - A special classification token ([CLS]) is appended to the sequence of patch embeddings. The output corresponding to this token from the final layer of the transformer encoder is used as the representation of the entire image for classification tasks.

 Architecture Overview

- **Input Image**: The image is divided into fixed-size patches.
- **Patch Embeddings**: Each patch is flattened and projected into a vector, forming a sequence of patch embeddings.
- **Positional Encodings**: Added to the patch embeddings to retain positional information.
- **Transformer Encoder**: Processes the sequence of embeddings using self-attention and feed-forward layers.
- **Classification Head**: The output corresponding to the [CLS] token is passed through a classifier to predict the class of the image.

 Advantages of Visual Transformers

1. **Scalability**:
   - ViTs can benefit from the scalability of transformers, which can handle large datasets and model sizes effectively. They scale well with increasing data and computational resources, often outperforming traditional convolutional neural networks (CNNs) when trained on large datasets.

2. **Flexibility**:
   - ViTs can be easily adapted to various vision tasks beyond classification, such as object detection and segmentation, by modifying the output layer and loss function accordingly.

3. **Global Context**:
   - The self-attention mechanism in transformers allows ViTs to capture long-range dependencies and global context within an image, which can be beneficial for understanding complex visual scenes.

 Challenges and Solutions

1. **Data Efficiency**:
   - ViTs typically require large amounts of training data to perform well because they lack the inductive biases inherent in CNNs (e.g., translation invariance and local connectivity).
   - To address this, techniques such as data augmentation, transfer learning, and pre-training on large datasets followed by fine-tuning on smaller datasets are commonly used.

2. **Computational Cost**:
   - The self-attention mechanism has a quadratic complexity with respect to the number of patches, making it computationally expensive for high-resolution images.
   - Hybrid models that combine CNNs (for initial feature extraction) with transformers (for high-level reasoning) have been explored to mitigate this issue.

 Applications

- **Image Classification**: ViTs have been used to achieve state-of-the-art performance on various image classification benchmarks.
- **Object Detection and Segmentation**: Extensions of ViTs, such as DETR (Detection Transformer), have been developed for object detection and segmentation tasks.
- **Vision-Language Tasks**: ViTs have been combined with NLP models to handle vision-language tasks like image captioning and visual question answering.

 Notable Visual Transformer Models

- **ViT (Visual Transformer)**: The original model introduced by Dosovitskiy et al.
- **DeiT (Data-efficient Image Transformers)**: An improved version of ViT that focuses on data efficiency and robustness, developed by researchers at Facebook AI.
- **Swin Transformer (Shifted Window Transformer)**: A hierarchical transformer model for vision tasks that uses shifted windows for efficient modeling of high-resolution images.

In summary, Visual Transformers represent a significant advancement in computer vision by leveraging the transformer architecture's ability to model complex dependencies and global context. While they require large datasets and computational resources, they offer a scalable and flexible alternative to traditional CNNs, with applications across a wide range of vision tasks.

\section{ALBEF}
The paper "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation" by Junnan Li et al. from Salesforce Research presents a new framework for vision-and-language pre-training (VLP) called ALBEF (Align BEfore Fuse). Here is an overview of the main concepts and innovations introduced in the paper:

 Key Concepts and Innovations:

1. **Problem with Existing Methods**:
   - Most existing VLP methods use a multimodal encoder to jointly model visual tokens (image features) and word tokens (text features).
   - These methods face challenges because the visual and word tokens are unaligned, making it difficult for the encoder to learn interactions between them.
   - Additionally, these methods often rely on expensive object detectors and high-resolution images, which can be computationally intensive and require extensive annotations.

2. **ALBEF Framework**:
   - **Alignment before Fusion**: ALBEF first aligns image and text representations before fusing them through cross-modal attention. This alignment is achieved using a contrastive loss, which helps to ground the vision and language representations.
   - **Detector-Free Encoding**: ALBEF uses a detector-free image encoder and a text encoder to independently process images and text before combining them with a multimodal encoder.
   - **Momentum Distillation (MoD)**: To handle noisy web data, ALBEF introduces momentum distillation, a self-training method that uses pseudo-targets generated by a momentum model (a slowly evolving teacher model) to guide the learning process.

3. **Pre-Training Objectives**:
   - **Image-Text Contrastive (ITC) Learning**: This loss aligns the representations from the unimodal encoders (image and text encoders) before they are fused.
   - **Masked Language Modeling (MLM)**: Similar to BERT, this objective involves predicting masked words in the text using both image and text context.
   - **Image-Text Matching (ITM)**: This objective determines whether a given image and text pair is a match, incorporating contrastive hard negative mining to improve the learning process.

4. **Theoretical Analysis**:
   - The paper provides a theoretical perspective, showing that ALBEF maximizes mutual information between different views of an image-text pair. Different training tasks (ITC, MLM) are interpreted as generating different views, enhancing the robustness and generalization of the learned representations.

5. **Performance**:
   - ALBEF achieves state-of-the-art performance on various downstream vision-language tasks, including image-text retrieval, visual question answering (VQA), and visual reasoning (NLVR2).
   - It outperforms existing methods that use much larger pre-training datasets and offers faster inference speeds.

6. **Implementation Details**:
   - ALBEF employs a 12-layer visual transformer (ViT-B/16) as the image encoder and a 6-layer transformer for both the text and multimodal encoders.
   - The pre-training dataset consists of large-scale image-text pairs, and the model is fine-tuned on specific downstream tasks.

 Summary
ALBEF introduces a novel approach to vision-and-language representation learning by aligning image and text representations before fusing them and leveraging momentum distillation to improve learning from noisy data. This framework not only addresses the limitations of previous methods but also achieves superior performance on multiple vision-language tasks.

\section{Relation and Sensitivity aware}
This author tackles to learn better multimodal representation by using the basic concept of ALBEF with two novel tasks: Relation aware and Sensitivity aware.