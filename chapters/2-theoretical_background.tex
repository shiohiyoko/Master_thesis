\chapter{Theoretical Background}
This section will describe the theoretical background for the techniques used for text encoder, image encoder, and cross attention methods. 
\section{Transformer}
A transformer is a type of deep learning model architecture that has revolutionized the field of natural language processing (NLP) and has been extended to various other domains such as computer vision and speech recognition. The transformer model was introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017. Here's an overview of its key components and how it works:

 Key Components of a Transformer

1. **Self-Attention Mechanism**:
   - **Self-attention** (or scaled dot-product attention) allows the model to weigh the importance of different words in a sentence relative to each other. It enables the model to focus on relevant parts of the input sequence when generating an output.
   - For each word, self-attention calculates a weighted sum of all the words in the sentence, where the weights are determined by the relevance of the words to each other.

2. **Multi-Head Attention**:
   - Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, the transformer splits the input into multiple heads and performs attention in parallel, then concatenates the results.

3. **Positional Encoding**:
   - Since transformers do not have a built-in notion of the order of elements in a sequence, positional encoding is added to the input embeddings to provide information about the position of each word in the sequence.

4. **Feed-Forward Neural Network**:
   - Each position in the sequence is processed independently through a feed-forward neural network. This is applied identically to each position, providing additional processing power to the model.

5. **Layer Normalization and Residual Connections**:
   - Layer normalization helps in stabilizing and accelerating the training process.
   - Residual connections (or skip connections) allow gradients to flow more easily through the network, which helps in training deep networks.

 Architecture Overview

A transformer model is composed of an encoder and a decoder, both of which are stacks of identical layers:

- **Encoder**: Each layer in the encoder consists of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The input to the encoder is a sequence of word embeddings, along with positional encodings.
  
- **Decoder**: Each layer in the decoder has three sub-layers: a multi-head self-attention mechanism, a multi-head attention mechanism over the encoder's output, and a position-wise fully connected feed-forward network. The decoder generates the output sequence by attending to both the previously generated tokens and the encoder's output.

 Advantages of Transformers

1. **Parallelization**:
   - Unlike recurrent neural networks (RNNs), transformers do not rely on sequential processing, allowing for much more parallelization during training, which significantly speeds up the process.

2. **Handling Long Dependencies**:
   - The self-attention mechanism allows transformers to handle long-range dependencies more effectively than RNNs and LSTMs, which can struggle with distant dependencies due to vanishing gradients.

3. **Scalability**:
   - Transformers have shown to scale well with the availability of large datasets and computational resources, leading to significant improvements in performance as the model size increases.

 Applications

- **Natural Language Processing**: Tasks such as translation, text summarization, and sentiment analysis.
- **Computer Vision**: Vision Transformers (ViTs) have been used for image classification and object detection.
- **Speech Recognition**: Transformers have been applied to model speech data and transcribe audio recordings.

 Popular Transformer Models

- **BERT (Bidirectional Encoder Representations from Transformers)**: Pre-trained on large text corpora to generate contextual embeddings for NLP tasks.
- **GPT (Generative Pre-trained Transformer)**: A series of models designed for generating coherent and contextually relevant text.
- **T5 (Text-To-Text Transfer Transformer)**: Converts all NLP tasks into a text-to-text format, providing a unified framework for diverse tasks.

In summary, the transformer model has become the foundation for many state-of-the-art approaches in NLP and beyond due to its ability to handle complex dependencies and its efficiency in training and inference.

% \section{ALBEF}
The paper "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation" by Junnan Li et al. from Salesforce Research presents a new framework for vision-and-language pre-training (VLP) called ALBEF (Align BEfore Fuse). Here is an overview of the main concepts and innovations introduced in the paper:

 Key Concepts and Innovations:

1. **Problem with Existing Methods**:
   - Most existing VLP methods use a multimodal encoder to jointly model visual tokens (image features) and word tokens (text features).
   - These methods face challenges because the visual and word tokens are unaligned, making it difficult for the encoder to learn interactions between them.
   - Additionally, these methods often rely on expensive object detectors and high-resolution images, which can be computationally intensive and require extensive annotations.

2. **ALBEF Framework**:
   - **Alignment before Fusion**: ALBEF first aligns image and text representations before fusing them through cross-modal attention. This alignment is achieved using a contrastive loss, which helps to ground the vision and language representations.
   - **Detector-Free Encoding**: ALBEF uses a detector-free image encoder and a text encoder to independently process images and text before combining them with a multimodal encoder.
   - **Momentum Distillation (MoD)**: To handle noisy web data, ALBEF introduces momentum distillation, a self-training method that uses pseudo-targets generated by a momentum model (a slowly evolving teacher model) to guide the learning process.

3. **Pre-Training Objectives**:
   - **Image-Text Contrastive (ITC) Learning**: This loss aligns the representations from the unimodal encoders (image and text encoders) before they are fused.
   - **Masked Language Modeling (MLM)**: Similar to BERT, this objective involves predicting masked words in the text using both image and text context.
   - **Image-Text Matching (ITM)**: This objective determines whether a given image and text pair is a match, incorporating contrastive hard negative mining to improve the learning process.

4. **Theoretical Analysis**:
   - The paper provides a theoretical perspective, showing that ALBEF maximizes mutual information between different views of an image-text pair. Different training tasks (ITC, MLM) are interpreted as generating different views, enhancing the robustness and generalization of the learned representations.

5. **Performance**:
   - ALBEF achieves state-of-the-art performance on various downstream vision-language tasks, including image-text retrieval, visual question answering (VQA), and visual reasoning (NLVR2).
   - It outperforms existing methods that use much larger pre-training datasets and offers faster inference speeds.

6. **Implementation Details**:
   - ALBEF employs a 12-layer visual transformer (ViT-B/16) as the image encoder and a 6-layer transformer for both the text and multimodal encoders.
   - The pre-training dataset consists of large-scale image-text pairs, and the model is fine-tuned on specific downstream tasks.

 Summary
ALBEF introduces a novel approach to vision-and-language representation learning by aligning image and text representations before fusing them and leveraging momentum distillation to improve learning from noisy data. This framework not only addresses the limitations of previous methods but also achieves superior performance on multiple vision-language tasks.
